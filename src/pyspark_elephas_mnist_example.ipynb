{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_elephas_mnist_example(working).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMRlNSSpl_E3"
      },
      "source": [
        "# prepare needed environment on Colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://apache.mirror.digionline.de/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XQ9-EnrqPfM"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lI1TMg2BmElu",
        "outputId": "9b590b44-3410-441b-e46c-f33e02b6aa20"
      },
      "source": [
        "!pip install elephas"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting elephas\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/8c/a459d974e01f782e2709c74c280030c0424115f022f33a887ced9e03282b/elephas-2.1.0.tar.gz\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from elephas) (0.29.23)\n",
            "Requirement already satisfied: tensorflow!=2.2.*,>=2 in /usr/local/lib/python3.7/dist-packages (from elephas) (2.5.0)\n",
            "Collecting keras==2.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/ba/2d058dcf1b85b9c212cc58264c98a4a7dd92c989b798823cc5690d062bb2/Keras-2.2.5-py2.py3-none-any.whl (336kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 24.7MB/s \n",
            "\u001b[?25hCollecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from elephas) (1.1.4)\n",
            "Collecting h5py==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.6MB/s \n",
            "\u001b[?25hCollecting pyspark<3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 89kB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (2.5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (3.12.4)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.34.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.12.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.12.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.4.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.15.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow!=2.2.*,>=2->elephas) (1.12)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->elephas) (1.4.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5->elephas) (3.13)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from hyperas->elephas) (5.1.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from hyperas->elephas) (1.0.0)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from hyperas->elephas) (0.1.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from hyperas->elephas) (5.6.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from hyperas->elephas) (0.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->elephas) (2.11.3)\n",
            "Collecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 57.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow!=2.2.*,>=2->elephas) (56.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (1.30.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (0.6.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas->elephas) (2.6.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas->elephas) (5.0.5)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas->elephas) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat->hyperas->elephas) (0.2.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas->elephas) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas->elephas) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas->elephas) (5.1.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas->elephas) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->hyperas->elephas) (7.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas->elephas) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas->elephas) (3.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas->elephas) (4.41.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt->hyperas->elephas) (2.5.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas->elephas) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas->elephas) (1.4.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas->elephas) (2.6.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas->elephas) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas->elephas) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->hyperas->elephas) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->elephas) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (4.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (1.3.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas->elephas) (0.10.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas->elephas) (1.5.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas->elephas) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->hyperas->elephas) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->hyperas->elephas) (1.0.18)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->hyperas->elephas) (5.5.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->hyperas->elephas) (22.0.3)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->hyperas->elephas) (1.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas->elephas) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->hyperas->elephas) (1.0.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt->hyperas->elephas) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->hyperas->elephas) (20.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->hyperas->elephas) (0.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (3.4.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow!=2.2.*,>=2->elephas) (3.1.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->hyperas->elephas) (0.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=5.2.0->notebook->jupyter->hyperas->elephas) (2.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas->elephas) (0.2.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->hyperas->elephas) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->hyperas->elephas) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-console->jupyter->hyperas->elephas) (0.7.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->hyperas->elephas) (2.4.7)\n",
            "Building wheels for collected packages: elephas, pyspark\n",
            "  Building wheel for elephas (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elephas: filename=elephas-2.1.0-cp37-none-any.whl size=27614 sha256=357be410e00c8638a7f3d544eb7252b8d9f2a7c2f68bd0ec9e186eb55d1fd2d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/8e/f7/afeaa15a424e0df01ff445dea2ac4cfddda282329494bbb027\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=fc7078f945c0ccb7481b4081e9b0879ffe51758b0b78b9e6e4b202ac05d24ed4\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built elephas pyspark\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: h5py, keras-applications, keras, hyperas, py4j, pyspark, elephas\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed elephas-2.1.0 h5py-2.10.0 hyperas-0.4.1 keras-2.2.5 keras-applications-1.0.8 py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4yJrTmugu4q",
        "outputId": "16c566a2-14f6-4964-cb7d-08c42497155d"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from elephas.spark_model import SparkModel\n",
        "from elephas.utils.rdd_utils import to_simple_rdd\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define basic parameters\n",
        "batch_size = 64\n",
        "nb_classes = 10\n",
        "epochs = 1\n",
        "\n",
        "# Create Spark context\n",
        "conf = SparkConf().setAppName('Mnist_Spark_MLP').setMaster('local[*]')\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, nb_classes)\n",
        "y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=784))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.1)\n",
        "model.compile(sgd, 'categorical_crossentropy', ['acc'])\n",
        "\n",
        "# Build RDD from numpy features and labels\n",
        "rdd = to_simple_rdd(sc, x_train, y_train)\n",
        "\n",
        "# Initialize SparkModel from tensorflow.keras model and Spark context\n",
        "spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')\n",
        "\n",
        "# Train Spark model\n",
        "spark_model.fit(rdd, epochs=epochs, batch_size=batch_size, verbose=0, validation_split=0.1)\n",
        "\n",
        "# Evaluate Spark model by evaluating the underlying model\n",
        "score = spark_model.evaluate(x_test, y_test, verbose=2)\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>> Fit model\n",
            " * Serving Flask app \"elephas.parameter.server\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://172.28.0.2:4000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>> Initialize workers\n",
            ">>> Distribute load\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "172.28.0.2 - - [31/May/2021 18:16:24] \"\u001b[37mGET /parameters HTTP/1.1\u001b[0m\" 200 -\n",
            "172.28.0.2 - - [31/May/2021 18:16:24] \"\u001b[37mGET /parameters HTTP/1.1\u001b[0m\" 200 -\n",
            "172.28.0.2 - - [31/May/2021 18:16:26] \"\u001b[37mPOST /update HTTP/1.1\u001b[0m\" 200 -\n",
            "172.28.0.2 - - [31/May/2021 18:16:26] \"\u001b[37mPOST /update HTTP/1.1\u001b[0m\" 200 -\n",
            "172.28.0.2 - - [31/May/2021 18:16:27] \"\u001b[37mGET /parameters HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ">>> Async training complete.\n",
            "Test accuracy: 0.9187291860580444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "UZiX5ZRtl_5y",
        "outputId": "5271eed2-1090-4e20-e286-f9f6b5a1cb23"
      },
      "source": [
        "'''\n",
        "Possible solution: https://stackoverflow.com/questions/61326144/pyspark-pipeline-fitdf-method-give-picklingerror-could-not-serialize-object  \n",
        "!pip install q keras==2.2.4\n",
        "!pip install q tensorflow==1.14.0\n",
        "---------------------------------------------------------------------------\n",
        "ValueError: Cell is empty\n",
        "\n",
        "\n",
        "ValueError                                Traceback (most recent call last)\n",
        "\n",
        "/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/serializers.py in dumps(self, obj)\n",
        "    436         try:\n",
        "--> 437             return cloudpickle.dumps(obj, pickle_protocol)\n",
        "    438         except pickle.PickleError:\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from elephas.ml_model import ElephasEstimator\n",
        "from elephas.ml.adapter import to_data_frame\n",
        "from elephas.utils.rdd_utils import to_simple_rdd\n",
        "\n",
        "\n",
        "# Define basic parameters\n",
        "batch_size = 64\n",
        "nb_classes = 10\n",
        "epochs = 1\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices\n",
        "y_train = to_categorical(y_train, nb_classes)\n",
        "y_test = to_categorical(y_test, nb_classes)\n",
        "\n",
        "spark = SparkSession\\\n",
        "        .builder\\\n",
        "        .appName(\"keras-elphas-mnist\")\\\n",
        "        .getOrCreate()\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=784))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(sgd, 'categorical_crossentropy', ['acc'])\n",
        "\n",
        "# Create Spark context\n",
        "#conf = SparkConf().setAppName('Mnist_Spark_MLP') # .setMaster('local[8]')\n",
        "#sc = SparkContext(conf=conf)\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Build RDD from numpy features and labels\n",
        "df = to_data_frame(sc, x_train, y_train, categorical=True)\n",
        "test_df = to_data_frame(sc, x_test, y_test, categorical=True)\n",
        "\n",
        "sgd_conf = optimizers.serialize(sgd)\n",
        "\n",
        "# Initialize Spark ML Estimator\n",
        "estimator = ElephasEstimator()\n",
        "estimator.set_keras_model_config(model.to_yaml())\n",
        "estimator.set_optimizer_config(sgd_conf)\n",
        "estimator.set_mode(\"synchronous\")\n",
        "estimator.set_loss(\"categorical_crossentropy\")\n",
        "estimator.set_metrics(['acc'])\n",
        "estimator.set_epochs(epochs)\n",
        "estimator.set_batch_size(batch_size)\n",
        "estimator.set_validation_split(0.1)\n",
        "estimator.set_categorical_labels(True)\n",
        "estimator.set_nb_classes(nb_classes)\n",
        "\n",
        "# Fitting a model returns a Transformer\n",
        "pipeline = Pipeline(stages=[estimator])\n",
        "fitted_pipeline = pipeline.fit(df)\n",
        "\n",
        "# Evaluate Spark model by evaluating the underlying model\n",
        "prediction = fitted_pipeline.transform(test_df)\n",
        "pnl = prediction.select(\"label\", \"prediction\")\n",
        "pnl.show(100)\n",
        "\n",
        "prediction_and_label = pnl.rdd.map(lambda row: (row.label, row.prediction))\n",
        "metrics = MulticlassMetrics(prediction_and_label)\n",
        "print(metrics.precision())\n",
        "print(metrics.recall())\n",
        "spark.stop()\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nPossible solution: https://stackoverflow.com/questions/61326144/pyspark-pipeline-fitdf-method-give-picklingerror-could-not-serialize-object  \\n!pip install q keras==2.2.4\\n!pip install q tensorflow==1.14.0\\n---------------------------------------------------------------------------\\nValueError: Cell is empty\\n\\n\\nValueError                                Traceback (most recent call last)\\n\\n/content/spark-3.1.1-bin-hadoop2.7/python/pyspark/serializers.py in dumps(self, obj)\\n    436         try:\\n--> 437             return cloudpickle.dumps(obj, pickle_protocol)\\n    438         except pickle.PickleError:\\n---------------------------------------------------------------------------\\n\\n\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\\nfrom tensorflow.keras.utils import to_categorical\\nfrom tensorflow.keras import optimizers\\n\\nfrom pyspark import SparkContext, SparkConf\\nfrom pyspark.mllib.evaluation import MulticlassMetrics\\nfrom pyspark.ml import Pipeline\\nfrom pyspark.sql import SparkSession\\n\\nfrom elephas.ml_model import ElephasEstimator\\nfrom elephas.ml.adapter import to_data_frame\\nfrom elephas.utils.rdd_utils import to_simple_rdd\\n\\n\\n# Define basic parameters\\nbatch_size = 64\\nnb_classes = 10\\nepochs = 1\\n\\n# Load data\\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\\n\\nx_train = x_train.reshape(60000, 784)\\nx_test = x_test.reshape(10000, 784)\\nx_train = x_train.astype(\"float32\")\\nx_test = x_test.astype(\"float32\")\\nx_train /= 255\\nx_test /= 255\\nprint(x_train.shape[0], \\'train samples\\')\\nprint(x_test.shape[0], \\'test samples\\')\\n\\n# Convert class vectors to binary class matrices\\ny_train = to_categorical(y_train, nb_classes)\\ny_test = to_categorical(y_test, nb_classes)\\n\\nspark = SparkSession        .builder        .appName(\"keras-elphas-mnist\")        .getOrCreate()\\n\\nmodel = Sequential()\\nmodel.add(Dense(128, input_dim=784))\\nmodel.add(Activation(\\'relu\\'))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(128))\\nmodel.add(Activation(\\'relu\\'))\\nmodel.add(Dropout(0.2))\\nmodel.add(Dense(10))\\nmodel.add(Activation(\\'softmax\\'))\\n\\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\\nmodel.compile(sgd, \\'categorical_crossentropy\\', [\\'acc\\'])\\n\\n# Create Spark context\\n#conf = SparkConf().setAppName(\\'Mnist_Spark_MLP\\') # .setMaster(\\'local[8]\\')\\n#sc = SparkContext(conf=conf)\\nsc = spark.sparkContext\\n\\n# Build RDD from numpy features and labels\\ndf = to_data_frame(sc, x_train, y_train, categorical=True)\\ntest_df = to_data_frame(sc, x_test, y_test, categorical=True)\\n\\nsgd_conf = optimizers.serialize(sgd)\\n\\n# Initialize Spark ML Estimator\\nestimator = ElephasEstimator()\\nestimator.set_keras_model_config(model.to_yaml())\\nestimator.set_optimizer_config(sgd_conf)\\nestimator.set_mode(\"synchronous\")\\nestimator.set_loss(\"categorical_crossentropy\")\\nestimator.set_metrics([\\'acc\\'])\\nestimator.set_epochs(epochs)\\nestimator.set_batch_size(batch_size)\\nestimator.set_validation_split(0.1)\\nestimator.set_categorical_labels(True)\\nestimator.set_nb_classes(nb_classes)\\n\\n# Fitting a model returns a Transformer\\npipeline = Pipeline(stages=[estimator])\\nfitted_pipeline = pipeline.fit(df)\\n\\n# Evaluate Spark model by evaluating the underlying model\\nprediction = fitted_pipeline.transform(test_df)\\npnl = prediction.select(\"label\", \"prediction\")\\npnl.show(100)\\n\\nprediction_and_label = pnl.rdd.map(lambda row: (row.label, row.prediction))\\nmetrics = MulticlassMetrics(prediction_and_label)\\nprint(metrics.precision())\\nprint(metrics.recall())\\nspark.stop()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}